{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is the leading library for NLP, and it has quickly become one of the most popular Python frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading spacy\n",
    "import spacy\n",
    "#loading English language model.\n",
    "# nlp = spacy.load('en')\n",
    "# or by\n",
    "from spacy.lang.en import English\n",
    "#create the nlp object\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization\n",
    "\n",
    "This returns a document object that contains tokens. A token is a unit of text in the document, such as individual words and punctuation. SpaCy splits contractions like \"don't\" into two tokens, \"do\" and \"n't\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "saw\n",
      "a\n",
      "saw\n",
      "that\n",
      "could\n",
      "n't\n",
      "saw\n",
      "any\n",
      "other\n",
      "saw\n",
      "I\n",
      "ever\n",
      "saw\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I saw a saw that couldn't saw any other saw I ever saw\")\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    # or print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through a document gives you token objects. Each of these tokens comes with additional information. In most cases, the important ones are token.lemma_ and token.is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing\n",
    "\n",
    "The first is \"lemmatizing.\" The \"lemma\" of a word is its base form. For example, \"walk\" is the lemma of the word \"walking\". So, when you lemmatize the word walking, you would convert it to walk.Lemmatizing similarly helps by combining multiple forms of the same word into one base form (\"calming\", \"calms\", \"calmed\" would all change to \"calm\").\n",
    "\n",
    "It's also common to remove stopwords. Stopwords are words that occur frequently in the language and don't contain much information. English stopwords include \"the\", \"is\", \"and\", \"but\", \"not\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token \t\tLemma \t\tStopword\n",
      "----------------------------------------\n",
      "I\t\tI\t\tTrue\n",
      "saw\t\tsaw\t\tFalse\n",
      "a\t\ta\t\tTrue\n",
      "saw\t\tsaw\t\tFalse\n",
      "that\t\tthat\t\tTrue\n",
      "could\t\tcould\t\tTrue\n",
      "n't\t\tnot\t\tTrue\n",
      "saw\t\tsaw\t\tFalse\n",
      "any\t\tany\t\tTrue\n",
      "other\t\tother\t\tTrue\n",
      "saw\t\tsaw\t\tFalse\n",
      "I\t\tI\t\tTrue\n",
      "ever\t\tever\t\tTrue\n",
      "saw\t\tsaw\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pattern Matching\n",
    "\n",
    "To match individual tokens, you create a Matcher. When you want to match a list of terms, it's easier and more efficient to use PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr = 'LOWER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you create a list of terms to match in the text. The phrase matcher needs the patterns as document objects. The easiest way to get these is with a list comprehension using the nlp model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['rainbow', 'rain', 'gotta', 'want']\n",
    "patterns = [nlp(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", None, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3766102292120407359, 8, 9), (3766102292120407359, 10, 11), (3766102292120407359, 13, 15), (3766102292120407359, 19, 20)]\n"
     ]
    }
   ],
   "source": [
    "text_doc = nlp(\"The way I see it, if you want the rainbow, you gotta put up with the rain.\") \n",
    "matches = matcher(text_doc)\n",
    "print(matches)\n",
    "#The matches here are a tuple of the match id and the positions of the start and end of the phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TerminologyList want\n"
     ]
    }
   ],
   "source": [
    "match_id, start, end = matches[0]\n",
    "print(nlp.vocab.strings[match_id], text_doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
